{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xYh20PSCRXT3",
    "outputId": "f48ed9cb-5bb0-47bd-b9eb-17a266a2c5b4"
   },
   "outputs": [],
   "source": [
    "'''@author: osamah Abdelhaq last edit date: 7/26/2020'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujzoEZr4zCTi"
   },
   "source": [
    "# Data Transformation + Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "fi3XBXIzU_YH",
    "outputId": "57359e56-cf69-4a44-f4ba-849a9d53b66c"
   },
   "outputs": [],
   "source": [
    "# !pip install -q -U watermark\n",
    "# !pip install -qq transformers\n",
    "# %reload_ext watermark\n",
    "# %watermark -v -p numpy,pandas,torch,transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "rxVQxQmItnvF",
    "outputId": "3cb65dd6-7fdc-4a0c-b9ec-39ce0c8135be"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#@title Setup & Config\n",
    "# import transformers\n",
    "# from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "# import torch\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# from collections import defaultdict\n",
    "# from textwrap import wrap\n",
    "\n",
    "# from torch import nn, optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "# RANDOM_SEED = 42\n",
    "# np.random.seed(RANDOM_SEED)\n",
    "# torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Qn6-JcauR_6"
   },
   "outputs": [],
   "source": [
    "#importing all data\n",
    "reddit_USPS = pd.read_pickle('reddit_USPS')\n",
    "reddit_ppp = pd.read_pickle('reddit_ppp')\n",
    "reddit_stim = pd.read_pickle('reddit_stim')\n",
    "reddit_une = pd.read_pickle('reddit_une')\n",
    "\n",
    "tiktok_USPS = pd.read_pickle('tiktok_USPS')\n",
    "tiktok_stim = pd.read_pickle('tiktok_stim')\n",
    "tiktok_une = pd.read_pickle('tiktok_une')\n",
    "\n",
    "fb_USPS = pd.read_csv('facebook_usps_frame.csv',index_col=0)\n",
    "fb_IRS = pd.read_csv('facebook_irs_frame.csv',index_col=0)\n",
    "fb_une = pd.read_csv('facebook_unemployment_frame.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dSrtDyzR80jX"
   },
   "outputs": [],
   "source": [
    "'''Editing date to be of format month/day/year'''\n",
    "\n",
    "#using pandas replace function\n",
    "dates = tiktok_USPS['date'].replace(['5d ago','4d ago','3d ago','2d ago','1d ago'],['6-8','6-9','6-10','6-11','6-12'])\n",
    "\n",
    "#using for loop to remedy other possible issues\n",
    "for index, date in enumerate(dates):\n",
    "    if 'ago' in date:\n",
    "        dates[index] = '6/12/2020'\n",
    "    elif 'K' in date:\n",
    "        dates[index] = '4/27/2020'\n",
    "    else:\n",
    "        #replacing dashes with slashes\n",
    "        date = date.replace('-' , '/') \n",
    "        #adding year\n",
    "        dates[index] = date+'/2020'\n",
    "\n",
    "tiktok_USPS['date'] = dates\n",
    "\n",
    "tiktok_USPS['date'] = pd.to_datetime(tiktok_USPS.date)\n",
    "\n",
    "#repeat for next use case\n",
    "dates = tiktok_stim['date'].replace(['5d ago','4d ago','3d ago','2d ago','1d ago'],['6/8','6/9','6/10','6/11','6/12'])\n",
    "\n",
    "for index, date in enumerate(dates):\n",
    "    if 'ago' in date:\n",
    "        dates[index] = '6/12/2020'\n",
    "    elif 'K' in date:\n",
    "        dates[index] = '4/27/2020'\n",
    "    else:\n",
    "        date = date.replace('-' , '/') \n",
    "        dates[index] = date+'/2020'\n",
    "\n",
    "tiktok_stim['date'] = dates\n",
    "\n",
    "tiktok_stim['date'] = pd.to_datetime(tiktok_stim.date)\n",
    "\n",
    "#repeat for next use case\n",
    "dates = tiktok_une['date'].replace(['5d ago','4d ago','3d ago','2d ago','1d ago'],['6/8','6/9','6/10','6/11','6/12'])\n",
    "\n",
    "for index, date in enumerate(dates):\n",
    "    if 'ago' in date:\n",
    "        dates[index] = '6/12/2020'\n",
    "    elif 'K' in date:\n",
    "        dates[index] = '4/27/2020'\n",
    "    else:\n",
    "        date = date.replace('-' , '/') \n",
    "        dates[index] = date+'/2020'\n",
    "\n",
    "tiktok_une['date'] = dates\n",
    "tiktok_une['date'] = pd.to_datetime(tiktok_une.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "craiUpf970We"
   },
   "outputs": [],
   "source": [
    "#renaming columns for ease of stacking\n",
    "tiktok_USPS.rename(columns = {'text':'Comments'}, inplace = True) \n",
    "tiktok_stim.rename(columns = {'text':'Comments'}, inplace = True)\n",
    "tiktok_une.rename(columns = {'text':'Comments'}, inplace = True)\n",
    "\n",
    "tiktok_une.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIQj3Pro46om"
   },
   "outputs": [],
   "source": [
    "'''Combining facebook data with tiktok data for polarity over time purposes'''\n",
    "#aligning data to be uniform\n",
    "fb_irs = fb_IRS.drop(['like','reply_to','day','month','dayofweek','subjectivity_label','polarity_label'], axis=1)\n",
    "fb_usps = fb_USPS.drop(['like','reply_to','day','month','dayofweek','subjectivity_label','polarity_label'], axis=1)\n",
    "fb_une = fb_une.drop(['like','reply_to','day','month','dayofweek','subjectivity_label','polarity_label'],axis=1)\n",
    "\n",
    "#renaming columns to match tiktok data set\n",
    "fb_irs.rename(columns = {'text':'Comments'}, inplace = True) \n",
    "fb_usps.rename(columns = {'text':'Comments'}, inplace = True) \n",
    "fb_une.rename(columns = {'text':'Comments'}, inplace = True) \n",
    "\n",
    "fb_irs.rename(columns = {'text_prep':'comment_ready'}, inplace = True) \n",
    "fb_usps.rename(columns = {'text_prep':'comment_ready'}, inplace = True) \n",
    "fb_une.rename(columns = {'text_prep':'comment_ready'}, inplace = True) \n",
    "\n",
    "columns_titles = [\"name\",\"Comments\",\"comment_ready\",\"date\",\"polarity\", \"subjectivity\"]\n",
    "\n",
    "fb_irs=fb_irs.reindex(columns=columns_titles)\n",
    "fb_usps=fb_usps.reindex(columns=columns_titles)\n",
    "fb_une=fb_une.reindex(columns=columns_titles)\n",
    "\n",
    "fb_une.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb_une.to_csv('fb_une_all.csv')\n",
    "# tiktok_une.to_csv('tiktok_une_all.csv')\n",
    "# fb_irs.to_csv('fb_irs_all.csv')\n",
    "# tiktok_stim.to_csv('tiktok_irs_all.csv')\n",
    "# tiktok_USPS.to_csv('tiktok_usps_all.csv')\n",
    "# fb_usps.to_csv('fb_usps_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "psfcIOe9--8U"
   },
   "outputs": [],
   "source": [
    "#stacking tiktok and facebook by use case\n",
    "une_frames = [fb_une, tiktok_une]\n",
    "usps_frames = [fb_usps, tiktok_USPS]\n",
    "irs_frames = [fb_irs,tiktok_stim]\n",
    "\n",
    "usps_data = pd.concat(usps_frames)\n",
    "une_data = pd.concat(une_frames)\n",
    "irs_data = pd.concat(irs_frames)\n",
    "\n",
    "usps_data['date'] = pd.to_datetime(usps_data['date'])\n",
    "une_data['date'] = pd.to_datetime(une_data['date'])\n",
    "irs_data['date'] = pd.to_datetime(irs_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktCqzmJ06gwF"
   },
   "outputs": [],
   "source": [
    "#***excludes reddit data for lack of date\n",
    "plt.figure(figsize=(20,5))\n",
    "ax = sns.lineplot(x = 'date', y = 'polarity', data = usps_data, ci=None)\n",
    "ax = sns.lineplot(x = 'date', y = 'polarity', data=irs_data, ci=None)\n",
    "ax = sns.lineplot(x = 'date', y = 'polarity', data=une_data, ci=None)\n",
    "ax.legend(labels=[\"USPS\",\"IRS\",\"Unemployment\"])\n",
    "ax.set_title(\"Average Polarity over Time\")\n",
    "ax.set_xlim(irs_data['date'].min(), irs_data['date'].max())\n",
    "\n",
    "\n",
    "#USPS polarity is more volatile\n",
    "\n",
    "'''Polarity is limited by lack of hashtag, gif, and external links interpretation'''\n",
    "print('**Important note: only a small number of USPS march comments, accounting for the polarity spike.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "une_data.head()\n",
    "une_data.to_csv('fb_tiktok_une.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0Vgl9Y_x88c"
   },
   "outputs": [],
   "source": [
    "#trying to identify usps_data outliers in polarization\n",
    "#uncomment code to print it all\n",
    "\n",
    "# dates = usps_data.date.to_numpy()\n",
    "# # for i in usps_data['date']:\n",
    "# #   if '2020-03' in i:\n",
    "# #     print(i)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# print(usps_data.loc[usps_data.polarity==1])\n",
    "\n",
    "#from seeing all march comments with polarity = 1.0, we can see that there are a limited number of samples (<5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJsgeIyIuh9g"
   },
   "outputs": [],
   "source": [
    "#preparing to combine datasets by use case with reddit datasets for topic modeling\n",
    "irs_data = irs_data.drop(['name','date'], axis=1)\n",
    "usps_data = usps_data.drop(['name','date'], axis=1)\n",
    "une_data = une_data.drop(['name','date'],axis=1)\n",
    "irs_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aolkf2YoukbB"
   },
   "outputs": [],
   "source": [
    "#confirming they match\n",
    "reddit_stim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_stim.to_csv('reddit_stimulus_all.csv')\n",
    "reddit_ppp.to_csv('reddit_paycheck_protection_all.csv')\n",
    "reddit_USPS.to_csv('reddit_usps_all.csv')\n",
    "reddit_une.to_csv('reddit_une_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqvlmeUGy7HH"
   },
   "outputs": [],
   "source": [
    "#combining datasets for each use case\n",
    "\n",
    "usps_frames = [reddit_USPS, usps_data] \n",
    "irs_frames = [reddit_ppp,irs_data, reddit_stim] \n",
    "une_frames = [une_data, reddit_une] \n",
    "\n",
    "usps_data = pd.concat(usps_frames)\n",
    "irs_data = pd.concat(irs_frames)\n",
    "une_data = pd.concat(une_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odvAxqZ4ZEUo"
   },
   "outputs": [],
   "source": [
    "# irs_data.to_csv(r'C:\\Users\\osama\\Desktop\\irs_data.csv')\n",
    "# usps_data.to_csv(r'C:\\Users\\osama\\Desktop\\usps_data.csv')\n",
    "# une_data.to_csv(r'C:\\Users\\osama\\Desktop\\une_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kydIsfzvaQXu"
   },
   "outputs": [],
   "source": [
    "#view basic information\n",
    "print('IRS Info:',irs_data.info()) #7,291 comments\n",
    "print('\\nUSPS Info:',usps_data.info()) #11,137 comments\n",
    "print('\\nUnemployment Info:',une_data.info()) #4,776 comments\n",
    "\n",
    "samp=len(irs_data)+len(usps_data)+len(une_data)\n",
    "\n",
    "print('\\nTotal Number of Comments =', samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fL5SydfAw4mo"
   },
   "outputs": [],
   "source": [
    "#finding where text preprocessing created null values and replacing with original comment\n",
    "irs_nulls = np.where(pd.isnull(irs_data['comment_ready']))\n",
    "usps_nulls = np.where(pd.isnull(usps_data['comment_ready']))\n",
    "une_nulls = np.where(pd.isnull(une_data['comment_ready']))\n",
    "\n",
    "#for loop to replace null with original via index\n",
    "for i in une_nulls[0]:\n",
    "    comm = une_data['Comments'].iloc[i]\n",
    "    une_data['comment_ready'].iloc[i] = comm \n",
    "\n",
    "#for loop to replace null with original via index\n",
    "for i in usps_nulls[0]:\n",
    "    comm = usps_data['Comments'].iloc[i]\n",
    "    usps_data['comment_ready'].iloc[i] = comm \n",
    "\n",
    "#for loop to replace null with original via index\n",
    "for i in irs_nulls[0]:\n",
    "    comm = irs_data['Comments'].iloc[i]\n",
    "    irs_data['comment_ready'].iloc[i] = comm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7SByv08gpfy"
   },
   "source": [
    "# Semi-Supervised Topic Modeling Using Correlation Explanation\n",
    "\n",
    "Step 1: Conduct Unsupervised model and examine topic, testing both large topic count and small topic count\n",
    "\n",
    "Step 2: Anchor Words to topics after the Unsupervised model\n",
    "\n",
    "Step 3: Experiment with anchor strength parameter and topic count parameter\n",
    "\n",
    "Step 4: Add to Dataframe\n",
    "\n",
    "**Keep in mind, no topic model will be 100% perfect, but a semi-supervised approach will greatly improve results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZsOonNLYNBg"
   },
   "outputs": [],
   "source": [
    "#necessary package\n",
    "!pip install corextopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y1zExZ2IymRX"
   },
   "source": [
    "### Unemployment Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-Yy-jP1AwC6"
   },
   "outputs": [],
   "source": [
    "#term frequencyâ€“inverse document frequency\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=.5,\n",
    "    min_df=10,\n",
    "    max_features=None,\n",
    "    ngram_range=(1, 2),\n",
    "    norm=None,\n",
    "    binary=True,\n",
    "    use_idf=False,\n",
    "    sublinear_tf=False\n",
    ")\n",
    "vectorizer = vectorizer.fit(une_data['comment_ready'])\n",
    "tfidf = vectorizer.transform(une_data['comment_ready'])\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uD4O7UAcvGLa"
   },
   "source": [
    "#### Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qGRG4bauBsC_"
   },
   "outputs": [],
   "source": [
    "#testing with 20 topics\n",
    "from corextopic import corextopic as ct\n",
    "model = ct.Corex(n_hidden=20)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab\n",
    ")\n",
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Jfs9e7til1h"
   },
   "outputs": [],
   "source": [
    "#testing with 5 topics\n",
    "model = ct.Corex(n_hidden=5)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab\n",
    ")\n",
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHXN7jTsnNdf"
   },
   "outputs": [],
   "source": [
    "#testing with 10 topics\n",
    "model = ct.Corex(n_hidden=10)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab\n",
    ")\n",
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "69BDoREOCXdF"
   },
   "outputs": [],
   "source": [
    "#testing with 15 topics\n",
    "model = ct.Corex(n_hidden=15)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab\n",
    ")\n",
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZygaBcIqytAI"
   },
   "source": [
    "#### Semi-Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLIo7QGcCuzW"
   },
   "outputs": [],
   "source": [
    "# Anchors designed to nudge the model towards measuring specific topics\n",
    "anchors = [\n",
    "    [\"march\", \"economic\", \"opportunity\", \"florida\"], #FLDEO\n",
    "    [\"submit\", \"question\", \"application\"], #application questions\n",
    "    [\"payment\", \"receive\", \"600\"], #unemployment payment\n",
    "    [\"eligible\", \"pending\", \"waiting\", \"file\", \"eligible\", \"still\"], #eligibility\n",
    "    [\"claim\",\"received\",\"week\"], #claim received\n",
    "    [\"deposit\", \"org\", \"floridajobs\"], #floridajobs\n",
    "    [\"call\", \"account\", \"two\", \"time\", \"information\"], #customer service calls\n",
    "    [\"covid\", \"virus\"], #covid\n",
    "    [\"benefit\",\"unemployment\",\"minimum wage\"], #minimum wage, unemployment benefits\n",
    "    [\"care\",\"desantis\",\"please\",\"fix\",\"help\",\"act\"], #Florida CARES Act/Governor Desantis\n",
    "    [\"work\",\"search\",\"company\"], #job search\n",
    "    [\"bank\",\"check\"], #direct deposit\n",
    "    [\"claim\", \"weeks\", \"able\"], #delayed claims\n",
    "    [\"election\", \"trump\", \"obama\"], #president, stock market\n",
    "    [\"google\"], #google, algorithm resume search\n",
    "    [\"stimulus\"] #phone customer service\n",
    "]\n",
    "anchors = [\n",
    "    [a for a in topic if a in vocab]\n",
    "    for topic in anchors\n",
    "]\n",
    "\n",
    "model = ct.Corex(n_hidden=18)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab,\n",
    "    anchors=anchors, # Pass the anchors in here\n",
    "    anchor_strength=4 # Tell the model how much it should rely on the anchors\n",
    ")\n",
    "\n",
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHEb8AnNiDS1"
   },
   "outputs": [],
   "source": [
    "topic_1 = 'fldeo'\n",
    "topic_2 = 'application questions'\n",
    "topic_3 = 'unemployment payment'\n",
    "topic_4 = 'eligibility'\n",
    "topic_5 = 'claim received'\n",
    "topic_6 = 'Florida Jobs'\n",
    "topic_7 = 'customer service'\n",
    "topic_8 = 'covid corona virus'\n",
    "topic_9 = 'unemployment benefits minimum wage'\n",
    "topic_10 = 'florida cares act governor desantis'\n",
    "topic_11 = 'job search'\n",
    "topic_12 = 'direct deposit'\n",
    "topic_13 = 'delayed claims'\n",
    "topic_14 = 'president Obama Trump Stock Market'\n",
    "topic_15 = 'google algorithm'\n",
    "topic_16 = 'phone customer servce'\n",
    "topic_17 = 'etc.'\n",
    "topic_18 = 'money payment'\n",
    "topic_19 = 'unidentified'\n",
    "\n",
    "topics = [topic_1,topic_2,topic_3,topic_4,topic_5,topic_6,topic_7,topic_8,topic_9,topic_10,\\\n",
    "          topic_11,topic_12,topic_13,topic_14,topic_15,topic_16,topic_17,topic_18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTpVa8C6C8CH"
   },
   "outputs": [],
   "source": [
    "#making new dataframe of topics to visualize data\n",
    "topic_une = pd.DataFrame(\n",
    "    model.transform(tfidf), \n",
    "    columns=topics\n",
    ").astype(int)\n",
    "topic_une.index = une_data.index\n",
    "df_une = pd.concat([une_data.Comments, topic_une], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4T_B5ubmKh8"
   },
   "outputs": [],
   "source": [
    "print(df_une)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOEoWRcKgkyE"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(topic_une)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V8YydwQsbpD4"
   },
   "outputs": [],
   "source": [
    "#counting amount of each topic for visualization\n",
    "counts = topic_une.sum(axis=0).to_numpy()\n",
    "# uni = topic_une.sum(axis=1).to_numpy()\n",
    "# uni = np.where(uni!=0,100,0)\n",
    "# uni = np.where(uni==0,1,uni)\n",
    "# uni = np.where(uni==100,0,uni)\n",
    "# uni_count = np.where(uni==0)\n",
    "# counts = np.append(counts,len(uni_count[0]))\n",
    "une_topic_counts = counts\n",
    "# topics = [topic_1,topic_2,topic_3,topic_4,topic_5,topic_6,topic_7,topic_8,topic_9,topic_10,\\\n",
    "#           topic_11,topic_12,topic_13,topic_14,topic_15,topic_16,topic_17,topic_18,topic_19]\n",
    "df_counts = pd.DataFrame({'Topic':topics,'Count':une_topic_counts})\n",
    "print('**Total number of comments:', samp)\n",
    "sns.barplot(x='Count',y='Topic',data=df_counts)\n",
    "\n",
    "# #important to note that sentences have multiple topics, there are misclassifications as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGKKsYQog_l2"
   },
   "outputs": [],
   "source": [
    "#using the last data frame to substitute the binary values with the actual topics\n",
    "#then appending it to the master data frame as a new column\n",
    "test = list(\n",
    "    model.transform(tfidf)\n",
    ")\n",
    "\n",
    "for i in range(len(test)):\n",
    "    tops = np.where(test[i]==True)\n",
    "    individual_topics = []\n",
    "    for j in tops[0]:\n",
    "        it = topics[j]\n",
    "        individual_topics.append(it)\n",
    "    test[i]=individual_topics\n",
    "\n",
    "une_data['topics']=test\n",
    "nested_topics = une_data['topics']\n",
    "topics_string = []\n",
    "for i in une_data['topics']:\n",
    "    topics_string.append(', '.join(i))\n",
    "une_data['topics'] = topics_string\n",
    "\n",
    "une_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4JCBh-MKW8Z2"
   },
   "outputs": [],
   "source": [
    "# #substituting classifications for polarity values\n",
    "topic_une['polarity'] = une_data['polarity']\n",
    "\n",
    "#counting each topic by positive, neutral, negative polarities\n",
    "positives = []\n",
    "\n",
    "for i in (topics):\n",
    "    pos = topic_une.loc[(topic_une[i] == 1) & (topic_une['polarity']>0.0)]\n",
    "    positives.append(pos.shape[0])\n",
    "\n",
    "negatives = []\n",
    "\n",
    "for i in (topics):\n",
    "    neg = topic_une.loc[(topic_une[i] == 1) & (topic_une['polarity']<0.0)]\n",
    "    negatives.append(neg.shape[0])\n",
    "\n",
    "neutral = []\n",
    "\n",
    "for i in topics:\n",
    "    net = topic_une.loc[(topic_une[i] == 1) & (topic_une['polarity']>-0.01) & (topic_une['polarity']<0.01)]\n",
    "    neutral.append(net.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9EZAtyh0pcq"
   },
   "outputs": [],
   "source": [
    "topic_polarity_matrix=pd.DataFrame({'Topics':topics,'positive':positives,'neutral':neutral,'negative':negatives,'count':une_topic_counts})\n",
    "topic_polarity_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9VzJ_tTLUpQO"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "x=topic_polarity_matrix.Topics\n",
    "fig = go.Figure(go.Bar(x=x, y=topic_polarity_matrix.positive, name='Positive Polarity'))\n",
    "fig.add_trace(go.Bar(x=x, y=topic_polarity_matrix.neutral, name='Neutral Polarity'))\n",
    "fig.add_trace(go.Bar(x=x, y=topic_polarity_matrix.negative, name='Negative Polarity'))\n",
    "\n",
    "fig.update_layout(barmode='stack')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bivariate Distribution: ')\n",
    "sns.jointplot(x=une_data.polarity, y=une_data.subjectivity);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk import ngrams, word_tokenize, pos_tag\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from itertools import chain #to unnest lists\n",
    "from wordcloud import WordCloud, ImageColorGenerator,STOPWORDS\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import gensim\n",
    "from gensim import matutils, models, corpora\n",
    "import scipy.sparse\n",
    "t = TweetTokenizer()\n",
    "def prep(frame):\n",
    "    '''input: column of dataframe\n",
    "    output: returns nested list of tokenized, lemmatized words with stop words removed'''\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lem = nltk.WordNetLemmatizer()\n",
    "    \n",
    "    #lemmatize words\n",
    "    #lowercase words\n",
    "    #remove stop words\n",
    "    tokens = []\n",
    "    for i in frame:\n",
    "        sentence = []\n",
    "        \n",
    "        #lemmatize based on the pos\n",
    "        # Get the single character pos constant from pos_tag like this:\n",
    "        tik = t.tokenize(i)\n",
    "        for tok in tik:\n",
    "            pos_label = (pos_tag(tok)[0][1][0]).lower()\n",
    "\n",
    "            # pos_refs = {'n': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "            #            'v': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "            #            'r': ['RB', 'RBR', 'RBS'],\n",
    "            #            'a': ['JJ', 'JJR', 'JJS']}\n",
    "            \n",
    "            if pos_label == 'j': pos_label = 'a'    # 'j' <--> 'a' reassignment\n",
    "\n",
    "            if pos_label in ['r']:  # For adverbs it's a bit different\n",
    "                try:\n",
    "                    s = wordnet.synset(tok+'.r.1').lemmas()[0].pertainyms()[0].name()\n",
    "                except:\n",
    "                    s = tok\n",
    "                    continue\n",
    "\n",
    "            elif pos_label in ['a', 's', 'v']: # For adjectives and verbs\n",
    "                try:\n",
    "                    s = lem.lemmatize(tok, pos=pos_label)\n",
    "                except:\n",
    "                    s = tok\n",
    "                    continue\n",
    "\n",
    "            else:   # For nouns and everything else as it is the default kwarg\n",
    "                try:\n",
    "                    s = lem.lemmatize(tok)\n",
    "                except:\n",
    "                    s = tok\n",
    "                    continue\n",
    "\n",
    "            if s not in stop_words:\n",
    "                #tokenize\n",
    "                sentence.append(s)\n",
    "            else:\n",
    "                del s\n",
    "            \n",
    "        tokens.append(sentence)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def alph(frame):      \n",
    "    '''input: takes a nested list of tokenized words\n",
    "    output: returns unnested list of tokens with letters, emojis, and punctuation removed.\n",
    "    Only alphabetical characters greater than length 4 are returned'''\n",
    "    #unnest list\n",
    "    unlist = list(chain(*frame))\n",
    "\n",
    "    #remove anything non alphabetical and append to une_alph\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    \n",
    "    alph = []\n",
    "    \n",
    "    #only keep words greater than 4 letters long\n",
    "    for i in unlist:\n",
    "        if (len(i) > 4 and i != \", \"):\n",
    "            alph.append(regex.sub('', i))\n",
    "            \n",
    "    return alph\n",
    "\n",
    "def freqChart(alph):\n",
    "    '''Input: word list, not nested, of only alphabetical words\n",
    "    Output: returns word frequency list(l) and graph of top 20 words (f)\n",
    "    returns least common words (u)\n",
    "    '''\n",
    "    \n",
    "    #create frequency distribution of words\n",
    "    fdist1 = FreqDist(alph)\n",
    "    \n",
    "    #ten most common words\n",
    "    l = fdist1.most_common(10)\n",
    "    \n",
    "    #frequency distribution chart of top 20 words\n",
    "    f = fdist1.plot(20, cumulative=True)\n",
    "    \n",
    "    #10 most infrequent words\n",
    "    u = fdist1.hapaxes()\n",
    "    \n",
    "    return l, u[:10], f\n",
    "\n",
    "def plot_word_cloud(alph):\n",
    "    '''input: word list, not nested, of only alphabetical words\n",
    "    output: plots word cloud'''\n",
    "    str_Cloud = \" \".join(alph)\n",
    "    \n",
    "    wc = WordCloud(# font_path='simsun.ttf',  \n",
    "               background_color=\"black\",  \n",
    "               width=600,height=300,\n",
    "               min_font_size=10,\n",
    "               stopwords=STOPWORDS,\n",
    "               max_words=2000,  \n",
    "               # mask=back_coloring, \n",
    "               max_font_size=120, \n",
    "               random_state=42,\n",
    "               collocations=False,\n",
    "               colormap = 'cool').generate(str_Cloud)\n",
    "    # image_colors = ImageColorGenerator(back_coloring)\n",
    "    plt.figure(figsize=(60,30))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"word_cloud.jpeg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''word frequency analysis'''\n",
    "#tokenize, lowercase, lemmatize, remove stop words\n",
    "une_tokens = prep(une_data['Comments'])\n",
    "\n",
    "#add tokens as a new column ('text_prep') to dataframe\n",
    "# unemployment_frame['text_prep'] = une_tokens\n",
    "\n",
    "#remove anything not alphabetical\n",
    "une_alph = alph(une_tokens)\n",
    "\n",
    "#fetch word frequencies\n",
    "une_freq_list, une_not_freq_list, une_freq_chart = freqChart(une_alph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print results\n",
    "print(une_freq_chart)\n",
    "print('Top Ten Most Common Words:',une_freq_list)\n",
    "print('Ten Least Common Words:',une_not_freq_list)\n",
    "# print('lexical diversity: ',lexical_diversity(une_alph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''word frequency analysis with bigrams'''\n",
    "\n",
    "#repeat but with bigrams for more insight\n",
    "une_bigrams = list(nltk.bigrams(une_alph))\n",
    "une_bigrams_freq_list, une_bigrams_uncommon, une_bigrams_graph = freqChart(une_bigrams)\n",
    "print(une_bigrams_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''word frequency analysis with trigrams'''\n",
    "\n",
    "#repeat but with trigrams for more insight\n",
    "une_trigrams = list(nltk.trigrams(une_alph))\n",
    "une_trigrams_freq_list, une_trigrams_uncommon, une_trigrams_graph = freqChart(une_trigrams)\n",
    "print(une_trigrams_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''word cloud'''\n",
    "\n",
    "plot_word_cloud(une_alph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-UjVrVwWXcJ"
   },
   "outputs": [],
   "source": [
    "# une_data.to_csv(r'une_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYNV2wXx211-"
   },
   "source": [
    "### USPS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNi-_Hyb3EPx"
   },
   "outputs": [],
   "source": [
    "vectorizer = vectorizer.fit(usps_data['comment_ready'])\n",
    "tfidf = vectorizer.transform(usps_data['comment_ready'])\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEhD-e6_20Yz"
   },
   "outputs": [],
   "source": [
    "#20 topics\n",
    "model = ct.Corex(n_hidden=20)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab\n",
    ")\n",
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7etuPCR3ZQy"
   },
   "outputs": [],
   "source": [
    "model = ct.Corex(n_hidden=10)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab\n",
    ")\n",
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YzbixOqw3dEw"
   },
   "outputs": [],
   "source": [
    "model = ct.Corex(n_hidden=15)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab\n",
    ")\n",
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p0j4sLBEBIh"
   },
   "source": [
    "### IRS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0BKvbo6DEE8p"
   },
   "outputs": [],
   "source": [
    "vectorizer = vectorizer.fit(irs_data['comment_ready'])\n",
    "tfidf = vectorizer.transform(irs_data['comment_ready'])\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2GdeNAKEIm1"
   },
   "outputs": [],
   "source": [
    "#20 topics\n",
    "model = ct.Corex(n_hidden=20)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab\n",
    ")\n",
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEJRQbkJYK6t"
   },
   "source": [
    "# Document Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2XEWzqUYHPr"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def _create_frequency_table(text_string) -> dict:\n",
    "\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text_string)\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    freqTable = dict()\n",
    "    for word in words:\n",
    "        word = ps.stem(word)\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "    return freqTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2VgPzBtXdb6u"
   },
   "outputs": [],
   "source": [
    "def _score_sentences(sentences, freqTable) -> dict:\n",
    "    \"\"\"\n",
    "    score a sentence by its words\n",
    "    Basic algorithm: adding the frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    sentenceValue = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        word_count_in_sentence = (len(word_tokenize(sentence)))\n",
    "        word_count_in_sentence_except_stop_words = 0\n",
    "        for wordValue in freqTable:\n",
    "            if wordValue in sentence.lower():\n",
    "                word_count_in_sentence_except_stop_words += 1\n",
    "                if sentence[:10] in sentenceValue:\n",
    "                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n",
    "                else:\n",
    "                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n",
    "\n",
    "        if sentence[:10] in sentenceValue:\n",
    "            sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] / word_count_in_sentence_except_stop_words\n",
    "\n",
    "        '''\n",
    "        Notice that a potential issue with our score algorithm is that long sentences will have an advantage over short sentences. \n",
    "        To solve this, we're dividing every sentence score by the number of words in the sentence.\n",
    "        \n",
    "        Note that here sentence[:10] is the first 10 character of any sentence, this is to save memory while saving keys of\n",
    "        the dictionary.\n",
    "        '''\n",
    "\n",
    "    return sentenceValue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PiESEU5mddjR"
   },
   "outputs": [],
   "source": [
    "def _find_average_score(sentenceValue) -> int:\n",
    "    \"\"\"\n",
    "    Find the average score from the sentence value dictionary\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    sumValues = 0\n",
    "    for entry in sentenceValue:\n",
    "        sumValues += sentenceValue[entry]\n",
    "\n",
    "    # Average value of a sentence from original text\n",
    "    average = (sumValues / len(sentenceValue))\n",
    "\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPged_0vdkI2"
   },
   "outputs": [],
   "source": [
    "def _generate_summary(sentences, sentenceValue, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] >= (threshold):\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eotgbjAGdoxF"
   },
   "outputs": [],
   "source": [
    "une_comments = []\n",
    "for comment in une_data.Comments:\n",
    "    une_comments.append(comment)\n",
    "\n",
    "separator = ', '\n",
    "une_all_comments = separator.join(une_comments)\n",
    "\n",
    "# 1 Create the word frequency table\n",
    "freq_table = _create_frequency_table(une_all_comments)\n",
    "\n",
    "'''\n",
    "We already have a sentence tokenizer, so we just need \n",
    "to run the sent_tokenize() method to create the array of sentences.\n",
    "'''\n",
    "\n",
    "# 2 Tokenize the sentences\n",
    "sentences = sent_tokenize(une_all_comments)\n",
    "\n",
    "# 3 Important Algorithm: score the sentences\n",
    "sentence_scores = _score_sentences(sentences, freq_table)\n",
    "\n",
    "# 4 Find the threshold\n",
    "threshold = _find_average_score(sentence_scores)\n",
    "\n",
    "# 5 Important Algorithm: Generate the summary\n",
    "summary = _generate_summary(sentences, sentence_scores, 3.0 * threshold)\n",
    "\n",
    "print('Unemployment Document Summarization')\n",
    "print('\\n',summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9crBrOhsetd8"
   },
   "outputs": [],
   "source": [
    "irs_comments = []\n",
    "for comment in irs_data.Comments:\n",
    "    irs_comments.append(comment)\n",
    "\n",
    "separator = ', '\n",
    "irs_all_comments = separator.join(irs_comments)\n",
    "\n",
    "# 1 Create the word frequency table\n",
    "freq_table = _create_frequency_table(irs_all_comments)\n",
    "\n",
    "'''\n",
    "We already have a sentence tokenizer, so we just need \n",
    "to run the sent_tokenize() method to create the array of sentences.\n",
    "'''\n",
    "\n",
    "# 2 Tokenize the sentences\n",
    "sentences = sent_tokenize(irs_all_comments)\n",
    "\n",
    "# 3 Important Algorithm: score the sentences\n",
    "sentence_scores = _score_sentences(sentences, freq_table)\n",
    "\n",
    "# 4 Find the threshold\n",
    "threshold = _find_average_score(sentence_scores)\n",
    "\n",
    "# 5 Important Algorithm: Generate the summary\n",
    "summary = _generate_summary(sentences, sentence_scores, 2.0 * threshold)\n",
    "\n",
    "print('IRS Document Summarization')\n",
    "print('\\n',summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AHnt_jBepriN"
   },
   "outputs": [],
   "source": [
    "usps_comments = []\n",
    "for comment in usps_data.Comments:\n",
    "    usps_comments.append(comment)\n",
    "\n",
    "separator = ', '\n",
    "usps_all_comments = separator.join(usps_comments)\n",
    "\n",
    "# 1 Create the word frequency table\n",
    "freq_table = _create_frequency_table(usps_all_comments)\n",
    "\n",
    "'''\n",
    "We already have a sentence tokenizer, so we just need \n",
    "to run the sent_tokenize() method to create the array of sentences.\n",
    "'''\n",
    "\n",
    "# 2 Tokenize the sentences\n",
    "sentences = sent_tokenize(usps_all_comments)\n",
    "\n",
    "# 3 Important Algorithm: score the sentences\n",
    "sentence_scores = _score_sentences(sentences, freq_table)\n",
    "\n",
    "# 4 Find the threshold\n",
    "threshold = _find_average_score(sentence_scores)\n",
    "\n",
    "# 5 Important Algorithm: Generate the summary\n",
    "summary = _generate_summary(sentences, sentence_scores, 3.5 * threshold)\n",
    "\n",
    "print('USPS Document Summarization')\n",
    "print('\\n',summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MhaKqK2VKAO"
   },
   "source": [
    "# Hypotheses Testing\n",
    "\n",
    "Unemployment:\n",
    "\n",
    "Users are:\n",
    "- dissatisfied with disbursement time/application processing time.\n",
    "- uncertain of eligibility requirements. \n",
    "- frustrated that unemployment pays more than minimum wage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0FcC6A-SVMtY"
   },
   "outputs": [],
   "source": [
    "# une_h3 = df_une.loc[(df_une['unemployment benefits minimum wage'] == 1)]\n",
    "# sample = une_h3['Comments'].sample(100)\n",
    "# #exporting and counting manually\n",
    "# sample.to_csv('H3 Sample.csv')\n",
    "\n",
    "une_h1 = df_une.loc[(df_une['eligibility'] == 1)]\n",
    "# sample = une_h1['Comments'].sample(100)\n",
    "# sample.to_csv('H1 Sample.csv')\n",
    "h1_TN = len(df_une.loc[(df_une['eligibility']==0)])\n",
    "h1_FN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KN72Oc-BhZQU"
   },
   "outputs": [],
   "source": [
    "une_h2 = df_une.loc[(df_une['delayed claims'] == 1)]\n",
    "# sample = une_h2['Comments'].sample(100)\n",
    "# sample.to_csv('H2 Sample.csv')\n",
    "len(une_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_TN = len(df_une.loc[df_une['delayed claims'] == 0])\n",
    "\n",
    "h2_FN = 0\n",
    "\n",
    "une_h3 = df_une.loc[(df_une['unemployment benefits minimum wage']==1)]\n",
    "h3_TN = len(df_une.loc[df_une['unemployment benefits minimum wage']==0])\n",
    "h3_FN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import math\n",
    "import scikitplot as skplt\n",
    "import copy\n",
    "\n",
    "def print_metrics(y_true,y_pred, TN):\n",
    "    '''As our classifier is more likely to misclassify a comment into a topic than it is to not detect the topic,\n",
    "    we can safely assume our TN count is equivalent to the sample of 100 from all 18 topics where this topic=0,\n",
    "    thus, the assumption is made the same ratio of false positives, are also the ratio of false negatives\n",
    "    the percentage of false positives to N samples is taken and applied to false positives to estimate false negatives'''\n",
    "    \n",
    "    conf = metrics.confusion_matrix(y_true, y_pred)\n",
    "    FP = conf[0][1]\n",
    "    \n",
    "    L = math.ceil((FP/len(y_true))*FP)\n",
    "    \n",
    "    z = TN - L\n",
    "    n = np.zeros(L,dtype=np.float64)\n",
    "    l = np.ones(L,dtype=np.float64)\n",
    "    negs = np.zeros(z,dtype=np.float64)\n",
    "    \n",
    "    y_true = np.concatenate((y_true,negs,l),axis=None)\n",
    "    y_pred = np.concatenate((y_pred,negs,n),axis=None)\n",
    "    \n",
    "    conf = metrics.confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    TP = conf[1][1]\n",
    "    FP = conf[0][1]\n",
    "    TN = conf[0][0]\n",
    "    FN = conf[1][0]\n",
    "    \n",
    "    precision = metrics.precision_score(y_true,y_pred)\n",
    "    recall = metrics.recall_score(y_true,y_pred)\n",
    "    f1 = (2*precision)/(precision+recall)\n",
    "    \n",
    "    print('Confusion Matrix')\n",
    "    print('True Negative: ', TN, '\\tFalse Positive:', FP, '\\nFalse Negative:', FN, '\\tTrue Positive:', TP)\n",
    "    \n",
    "    print('\\nPrecision Score:', precision)\n",
    "    print('Recall Score:', recall)\n",
    "    print('F1 Score:',f1)\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "#     plt.xlim([0, 1])\n",
    "#     plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "h2_y_true = np.array([1,1,1,1,1,0,1,1,1,1,0,1,1,0,1,1,0,0,1,1,1,0,0,1,1,1,0,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,0,1,1,0,1,1,1,1,1,1,0,1,0,1,1,1,1,0,1,0,0,1,0,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,0,0,0,0,1,1,0,1,1,1,1,1],dtype=np.float64)\n",
    "h2_y_pred = np.ones(100*math.ceil(len(une_h2)/100))\n",
    "h2_y_true = np.repeat(h2_y_true,math.ceil(len(une_h2)/100))\n",
    "\n",
    "#extrapolating predictions\n",
    "print('Hypothesis 2 Evaluation: Delayed Claims')\n",
    "print_metrics(h2_y_true,h2_y_pred, h2_TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_y_true = np.array([1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,1,0,1,0,0,1,0,0,0,1,1,0,0,1,1,1,0,0,0,0,1,1,0,0,0,0,1])\n",
    "h3_y_pred = np.ones(100*math.ceil(len(une_h3)/100))\n",
    "h3_y_true = np.repeat(h3_y_true,math.ceil(len(une_h3)/100))\n",
    "\n",
    "#extrapolating predictions\n",
    "print('Hypothesis 3 Evaluation: Minimum Wage compared to Unemployment')\n",
    "print_metrics(h3_y_true,h3_y_pred, h3_TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_y_true = np.array([1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,0,0,1,0,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,1,1,1],dtype=np.float64)\n",
    "h1_y_pred = np.ones(100*math.ceil(len(une_h1)/100))\n",
    "h1_y_true = np.repeat(h1_y_true,math.ceil(len(une_h1)/100))\n",
    "\n",
    "#extrapolating predictions\n",
    "print('Hypothesis 1 Evaluation: Eligibility Clarification')\n",
    "print_metrics(h1_y_true,h1_y_pred, h1_TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_une_data = pd.read_csv('excel_une_data.csv',index_col=0)\n",
    "excel_une_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = []\n",
    "for i in excel_une_data['topics']:\n",
    "    l = i.replace('[', ' ')\n",
    "    t = l.replace(']', ' ')\n",
    "    tops.append(t.split(','))\n",
    "print(tops[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = pd.DataFrame(tops)\n",
    "tops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops.to_csv('topics_in_columns.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ujzoEZr4zCTi",
    "g7SByv08gpfy",
    "y1zExZ2IymRX",
    "uD4O7UAcvGLa",
    "ZygaBcIqytAI",
    "bYNV2wXx211-",
    "6p0j4sLBEBIh",
    "oEJRQbkJYK6t"
   ],
   "name": "Topic_Modeling/Document_Summarization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
