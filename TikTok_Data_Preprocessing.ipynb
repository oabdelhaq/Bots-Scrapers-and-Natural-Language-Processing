{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TikTok Data Preprocessing\n",
    "\n",
    "In this notebook, the tiktok data for all three datasets is imported from json format then cleaned, tidied up, and saved as both a pickle object and a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk import *\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@author osamah Abdelhaq last edit: 6/17/2020\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "@author osamah Abdelhaq last edit: 6/17/2020\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete every \"New Post\" column\n",
    "def NewPost(data):\n",
    "    j=0\n",
    "    while j < len(data):\n",
    "        del data[j]\n",
    "        j+=1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(data):\n",
    "    splits = []\n",
    "    '''\n",
    "    deletes unwanted data\n",
    "    splits has every post split by \\n\n",
    "    split[0] returns the first post\n",
    "    split[0][1] returns the first sentence of the first post \n",
    "    '''\n",
    "    #replace these irrelevant words/hyperlinks and replace with a space\n",
    "    #then split by \\n\n",
    "    i=1\n",
    "    for i in range(len(data)):\n",
    "        data[i]=re.sub('Suggested accounts','',data[i])\n",
    "        data[i]=re.sub('Report','',data[i])\n",
    "        data[i]=re.sub('Follow','',data[i])\n",
    "        data[i]=re.sub('Hide','',data[i])\n",
    "        data[i]=re.sub('videos','',data[i])\n",
    "        data[i] = re.sub('View more','',data[i])\n",
    "        data[i] = re.sub('\\\\\\w{5}\\w?','',data[i])\n",
    "        sent = data[i].strip().split('\\n')\n",
    "        splits.append(sent)\n",
    "        i+=1\n",
    "\n",
    "    j=0\n",
    "    while j < len(splits):\n",
    "        #ignore arbitrary page labeling\n",
    "        del(splits[j][0:3])\n",
    "        del(splits[j][-50:])\n",
    "        j+=1\n",
    "        \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting arbitrary columns\n",
    "def arbitrary(splits):\n",
    "    '''returns list of data without arbitrary website information'''\n",
    "    del splits[10]\n",
    "    del splits[12-1]\n",
    "    del splits[14-2]\n",
    "    del splits[17-3]\n",
    "    del splits[26-4]\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoInformation(post):\n",
    "    '''\n",
    "    returns list of videoInformation\n",
    "    first dimension index returns video information of that post. ie. video_information[0] = post[0]\n",
    "    includes: creator username, post date, caption, and view count\n",
    "    this function assumes the first K in the dataset is the views, exceptions have been fixed manually\n",
    "    '''\n",
    "    idx = [i for i, s in enumerate(post) if 'K' in s]\n",
    "    inf = post[:idx[0]+1]\n",
    "    return inf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getComments(splits,video_information):\n",
    "    '''separates video information from the comments'''\n",
    "    all_comments=[]\n",
    "    for i in range(len(splits)):\n",
    "        idx = len(video_information[i])\n",
    "        post_comments = splits[i][idx:]\n",
    "        all_comments.append(post_comments)\n",
    "    return all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emptyStrings(all_comments):\n",
    "    '''removes empty strings'''\n",
    "    #remove empty strings\n",
    "    all_content=[]\n",
    "    t=0\n",
    "    x=0\n",
    "    for t in range(len(all_comments)):\n",
    "        for x in range(len(all_comments[t])):\n",
    "            if (all_comments[t][x]!=''):\n",
    "                all_content.append(all_comments[t][x])\n",
    "            x+=1\n",
    "        t+=1\n",
    "    return all_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_Frame(all_content):\n",
    "    '''returns dataframe by separating into name, comment, and date'''\n",
    "    name = []\n",
    "    comment = []\n",
    "    date = []\n",
    "\n",
    "    name.clear()\n",
    "    comment.clear()\n",
    "    date.clear()\n",
    "\n",
    "    for i in range(0,len(all_content),3):\n",
    "        n = all_content[i]\n",
    "        if ('View more' and 'View replies' not in n):\n",
    "            name.append(n)\n",
    "\n",
    "    for j in range(1,len(all_content),3):\n",
    "        c = all_content[j]\n",
    "        if ('View more' and 'View replies' not in c):\n",
    "            comment.append(c)\n",
    "\n",
    "    for k in range(2,len(all_content),3):\n",
    "        d = all_content[k]\n",
    "        if ('View more' and 'View replies' not in d):    \n",
    "            date.append(d)\n",
    "    \n",
    "    frame = pd.DataFrame({'name':name,'text':comment,'date':date})\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "with open('Tik_Tok_Stimulus_Check') as f:\n",
    "    content = json.load(f)\n",
    "\n",
    "'''\n",
    "returns data frame of data with three columns, name, text, and date\n",
    "returns video_information list that contains information on each video\n",
    "first dimension index returns video information of that post. ie. video_information[0] = post[0]\n",
    "includes: creator username, post date, caption, and view count\n",
    "\n",
    "'''\n",
    "    \n",
    "#delete where 'New Post' is marked\n",
    "content = NewPost(content)\n",
    "\n",
    "#parse data\n",
    "content = splits(content)\n",
    "\n",
    "#remove unintentionally collected, irrelevant data\n",
    "content = arbitrary(content)\n",
    "\n",
    "#gathering video information\n",
    "stimulus_check_video_information = []\n",
    "\n",
    "i=0\n",
    "\n",
    "for i in range(len(content)):\n",
    "    inf=videoInformation(content[i])\n",
    "    stimulus_check_video_information.append(inf)\n",
    "    \n",
    "#parse comments from video information    \n",
    "comments = getComments(content,stimulus_check_video_information)\n",
    "\n",
    "#remove empty strings\n",
    "all_content = emptyStrings(comments)\n",
    "\n",
    "#correcting data parsed incorrectly\n",
    "all_content[4050] = ' '.join(all_content[4050:4053])\n",
    "\n",
    "all_content[4050] = ' '.join(all_content[4050:4051])\n",
    "\n",
    "del all_content[4051]\n",
    "del all_content[4051]\n",
    "\n",
    "#deleting any incidents where view replies was not clicked during the scraping process\n",
    "idx=[i for i, s in enumerate(all_content) if 'View replies' in s]\n",
    "counter = 0\n",
    "for dex in idx:\n",
    "    del all_content[dex-counter]\n",
    "    counter+=1\n",
    "    \n",
    "#correcting parsing errors\n",
    "all_content[7186] = ' '.join(all_content[7186:7188])\n",
    "\n",
    "del all_content[7187]\n",
    "del all_content[7187]\n",
    "\n",
    "del all_content[6357]\n",
    "del all_content[6357]\n",
    "\n",
    "all_content[6490] = ' '.join(all_content[6490:6492])\n",
    "\n",
    "del all_content[6491]\n",
    "\n",
    "del all_content[7320:7322]\n",
    "\n",
    "del all_content[7625]\n",
    "\n",
    "del all_content[7659]\n",
    "\n",
    "del all_content[7848:7850]\n",
    "\n",
    "stimulus_check_frame = pd_Frame(all_content)\n",
    "\n",
    "#save dataframe as pickle object\n",
    "stimulus_check_frame.to_pickle('stimulus_tiktok_dataframe.p')\n",
    "\n",
    "'''\n",
    "emojis do not translate into csv, become converted into special characters\n",
    "'''\n",
    "stimulus_check_frame.to_csv(r'C:\\Users\\osama\\Desktop\\Stimulus_Check_DF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2913 entries, 0 to 2912\n",
      "Data columns (total 3 columns):\n",
      "name    2913 non-null object\n",
      "text    2913 non-null object\n",
      "date    2913 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 68.4+ KB\n",
      "None\n",
      "                   name                                               text  \\\n",
      "0  HI if youâ€™re viewing  lmao bruh Iâ€™m smacked and I watched this and c...   \n",
      "1         Ed Edd n Eddy                                     I felt that ðŸ’€ðŸ’€   \n",
      "2                    <3                                      @thugg_waffle   \n",
      "3                     ðŸ¦ˆ                             This is the best video   \n",
      "4            Ian Daniel                                               Same   \n",
      "\n",
      "   date  \n",
      "0  4-19  \n",
      "1  4-21  \n",
      "2  4-27  \n",
      "3  4-27  \n",
      "4  4-28  \n"
     ]
    }
   ],
   "source": [
    "print(stimulus_check_frame.info())\n",
    "print(stimulus_check_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "with open('Tik_Tok_Unemployment') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "\n",
    "'''\n",
    "returns data frame of data with three columns, name, text, and date\n",
    "returns video_information list that contains information on each video\n",
    "first dimension index returns video information of that post. ie. video_information[0] = post[0]\n",
    "includes: creator username, post date, caption, and view count\n",
    "\n",
    "'''\n",
    "   \n",
    "#delete where 'New Post' is marked\n",
    "data = NewPost(data)\n",
    "\n",
    "#parse the data\n",
    "data = splits(data)\n",
    "\n",
    "#remove unintentionally collected, irrelevant data\n",
    "del data[1]\n",
    "del data[3-1]\n",
    "del data[6-2]\n",
    "del data[11-3]\n",
    "del data[16-4]\n",
    "del data[21-5]\n",
    "\n",
    "#gathering video information\n",
    "#somewhat automated but errors in data collection forced me to manually slice some of the data\n",
    "unemployment_video_information = []\n",
    "\n",
    "for i in range(9):\n",
    "    inf=videoInformation(data[i])\n",
    "    unemployment_video_information.append(inf)\n",
    "    \n",
    "idx = 6\n",
    "unemployment_video_information.append(data[9][:idx+1])\n",
    "unemployment_video_information.append(data[10][:idx])\n",
    "\n",
    "for i in range(11,16):\n",
    "    inf=videoInformation(data[i])\n",
    "    unemployment_video_information.append(inf)\n",
    "\n",
    "unemployment_video_information.append([16][:idx])\n",
    "unemployment_video_information.append([17][:idx])\n",
    "unemployment_video_information.append(data[18][:idx])\n",
    "unemployment_video_information.append(data[19][:idx])\n",
    "unemployment_video_information.append(data[20][:idx+1])\n",
    "unemployment_video_information.append(data[21][:idx+1])\n",
    "unemployment_video_information.append(data[22][:idx])\n",
    "unemployment_video_information.append(data[23][:idx])\n",
    "\n",
    "#parse comments from the video information\n",
    "comment = getComments(data,unemployment_video_information)\n",
    "\n",
    "#remove empty strings\n",
    "all_cont = emptyStrings(comment)\n",
    "\n",
    "#deleting any incidents where view replies was not clicked during the scraping process\n",
    "indx=[i for i, s in enumerate(all_cont) if 'View replies' in s]\n",
    "\n",
    "count = 0\n",
    "\n",
    "for dex in indx:\n",
    "    del all_cont[dex-count]\n",
    "    count+=1\n",
    "    \n",
    "#correcting parsing errors\n",
    "del all_cont[434]\n",
    "\n",
    "all_cont[2341] = ' '.join(all_cont[2341:2343])\n",
    "\n",
    "del all_cont[2342]\n",
    "\n",
    "del all_cont[2997:3001]\n",
    "\n",
    "del all_cont[3045:3047]\n",
    "\n",
    "del all_cont[3081:3085]\n",
    "\n",
    "all_cont[3226] = ' '.join(all_cont[3226:3228])\n",
    "\n",
    "del all_cont[3227]\n",
    "\n",
    "all_cont[3259] = ' '.join(all_cont[3259:3262])\n",
    "\n",
    "del all_cont[3260:3262]\n",
    "\n",
    "del all_cont[3541]\n",
    "\n",
    "#convert into dataframe\n",
    "unemployment_frame = pd_Frame(all_cont)\n",
    "\n",
    "#save dataframe as pickle object\n",
    "unemployment_frame.to_pickle('unemployment_tiktok_dataframe.p')\n",
    "\n",
    "'''\n",
    "emojis do not translate into csv, become converted into special characters\n",
    "'''\n",
    "unemployment_frame.to_csv(r'C:\\Users\\osama\\Desktop\\Unemployment_DF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1898 entries, 0 to 1897\n",
      "Data columns (total 3 columns):\n",
      "name    1898 non-null object\n",
      "text    1898 non-null object\n",
      "date    1898 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 44.6+ KB\n",
      "None\n",
      "                          name  \\\n",
      "0         Heather MacIvor Moul   \n",
      "1                        Peace   \n",
      "2         Heather MacIvor Moul   \n",
      "3  Cathryn Patterson Â· Creator   \n",
      "4            user1156407460130   \n",
      "\n",
      "                                                text  date  \n",
      "0                         cannot hear you bc lashes.   4-8  \n",
      "1  If you close your eyes they turn all the way d...   4-8  \n",
      "2                                          good tip!  4-13  \n",
      "3                                         thank you!  4-13  \n",
      "4  Lol stop sheâ€™s giving good tips and looks so g...  4-19  \n"
     ]
    }
   ],
   "source": [
    "print(unemployment_frame.info())\n",
    "print(unemployment_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "with open('Tik_Tok_USPS') as f:\n",
    "    collection = json.load(f)\n",
    "    \n",
    "''''\n",
    "returns data frame of data with three columns, name, text, and date\n",
    "returns video_information list that contains information on each video\n",
    "first dimension index returns video information of that post. ie. video_information[0] = post[0]\n",
    "includes: creator username, post date, caption, and view count\n",
    "'''\n",
    "\n",
    "#parse the data\n",
    "collection = NewPost(collection)\n",
    "collection = splits(collection)\n",
    "\n",
    "#remove unintentionally collected, irrelevant data\n",
    "del (collection[1][0])\n",
    "del collection[0]\n",
    "del collection[13-1]\n",
    "del collection[17-2]\n",
    "del collection[20-3]\n",
    "del collection[26-4]\n",
    "\n",
    "#gathering video information\n",
    "#somewhat automated but errors in data collection forced me to manually slice some of the data\n",
    "usps_video_information = []\n",
    "\n",
    "usps_video_information.append(videoInformation(collection[0]))\n",
    "usps_video_information.append(collection[1][:6])\n",
    "\n",
    "for i in range(2,5):\n",
    "    inf=videoInformation(collection[i])\n",
    "    usps_video_information.append(inf)\n",
    "\n",
    "usps_video_information.append(collection[5][:6])\n",
    "usps_video_information.append(collection[6][:6])\n",
    "usps_video_information.append(collection[7][:6])\n",
    "\n",
    "for i in range(8,11):\n",
    "    inf=videoInformation(collection[i])\n",
    "    usps_video_information.append(inf)\n",
    "\n",
    "usps_video_information.append(collection[11][:6])\n",
    "\n",
    "for i in range(12,14):\n",
    "    inf=videoInformation(collection[i])\n",
    "    usps_video_information.append(inf)\n",
    "\n",
    "usps_video_information.append(collection[14][:6])\n",
    "\n",
    "for i in range(15,22):\n",
    "    inf=videoInformation(collection[i])\n",
    "    usps_video_information.append(inf)\n",
    "\n",
    "usps_video_information.append(collection[22][:6])\n",
    "usps_video_information.append(collection[23][:6])\n",
    "usps_video_information.append(collection[24][:6])\n",
    "\n",
    "#parse comments from the video information\n",
    "comment = getComments(collection,usps_video_information)\n",
    "\n",
    "#remove empty strings\n",
    "all_collection = emptyStrings(comment)\n",
    "\n",
    "#deleting any incidents where view replies was not clicked during the scraping process\n",
    "idex=[i for i, s in enumerate(all_collection) if 'View replies' in s]\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in idex:\n",
    "    del all_collection[i-count]\n",
    "    count+=1\n",
    "    \n",
    "#correcting parsing errors\n",
    "del all_collection[351:353]\n",
    "\n",
    "all_collection[2758] = ' '.join(all_collection[2758:2760])\n",
    "del all_collection[2759]\n",
    "\n",
    "del all_collection[2925:2927]\n",
    "\n",
    "del all_collection[3210:3212]\n",
    "\n",
    "del all_collection[3255:3257]\n",
    "\n",
    "#convert into dataframe\n",
    "usps_frame = pd_Frame(all_collection)\n",
    "\n",
    "#save dataframe as pickle object\n",
    "usps_frame.to_pickle('usps_tiktok_dataframe.p')\n",
    "\n",
    "'''\n",
    "emojis do not translate into csv, become converted into special characters\n",
    "'''\n",
    "usps_frame.to_csv(r'C:\\Users\\osama\\Desktop\\USPS_DF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1335 entries, 0 to 1334\n",
      "Data columns (total 3 columns):\n",
      "name    1335 non-null object\n",
      "text    1335 non-null object\n",
      "date    1335 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 31.4+ KB\n",
      "None\n",
      "                  name                                               text  \\\n",
      "0  U.S. Postal Service  Alright thatâ€™s pretty cool, how do we apply th...   \n",
      "1                Alvez                 isn't Trump gonna dissolve y'all??   \n",
      "2                C.Erk                                             Dead ðŸ˜‚   \n",
      "3    UhaveAlotOfTalent  Hopefully pos postal service private is way be...   \n",
      "4                 Andy  Honestly if showed this to trump supporters th...   \n",
      "\n",
      "   date  \n",
      "0  5-13  \n",
      "1  5-14  \n",
      "2  5-14  \n",
      "3  5-14  \n",
      "4  5-14  \n"
     ]
    }
   ],
   "source": [
    "print(usps_frame.info())\n",
    "print(usps_frame.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
