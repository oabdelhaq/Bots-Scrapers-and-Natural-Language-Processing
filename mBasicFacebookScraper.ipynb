{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@author: osamah Abdelhaq last edit: 6/29/2020\n",
    "'''\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Crawler\n",
    "\n",
    "Set to scrape information from about USPS's 80 most recent posts, FLDEO's 50 most recent posts, and IRS's 106 most recent posts.\n",
    "\n",
    "Uses provided mbasic facebook link to a page to scrape posts, comments, replies, likes on comments and replies, and timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacebookBot(webdriver.PhantomJS):\n",
    "    \"\"\"Class for browsing facebook\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.browser = webdriver.Chrome(executable_path=\"C:\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "    def login(self, email, password):\n",
    "        '''Log to facebook using email and password'''\n",
    "        url = \"https://mbasic.facebook.com\"\n",
    "        self.browser.get(url)\n",
    "        email_element = self.browser.find_element_by_name(\"email\")\n",
    "        email_element.send_keys(email)\n",
    "        pass_element = self.browser.find_element_by_name(\"pass\")\n",
    "        pass_element.send_keys(password)\n",
    "        pass_element.send_keys(Keys.ENTER)\n",
    "        if self.browser.find_element_by_class_name(\"bi\"):\n",
    "            self.browser.find_element_by_class_name(\"bp\").click();\n",
    "        try:\n",
    "            self.browser.find_element_by_name(\"xc_message\")\n",
    "            print(\"Logged in\")\n",
    "            return True\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"Fail to login\")\n",
    "            return False\n",
    "                \n",
    "    def getContent(self, url, deep):\n",
    "        '''\n",
    "        Get a list of comments (list:Post) in post url(str) iterating deep(int) times in the page\n",
    "        \n",
    "        Collects the post content and likes, followed by all comments including user name, timestamp, and likes.\n",
    "        \n",
    "        After collecting comments on that page, it collects all reply threads with more than one reply.\n",
    "        \n",
    "        Then it finds more comments on that post until no comments remain.\n",
    "        \n",
    "        Repeats for deep (int) amount of posts. 6 deep equals 24 posts.\n",
    "        \n",
    "        Note: unicode is scraped, some comments are repeated when involved in reply conversation.\n",
    "        '''\n",
    "\n",
    "        self.browser.get(url)\n",
    "        \n",
    "        content = []\n",
    "        \n",
    "        #gathers links to the posts\n",
    "        links = []\n",
    "        for i in range(deep):\n",
    "            for a in self.browser.find_elements_by_xpath('.//a'):\n",
    "                try:\n",
    "                    l = a.get_attribute('href')\n",
    "                    \n",
    "                    #****change to match source code link\n",
    "                    if ('/IRS/photos/a.' in l):\n",
    "                        if (l[-4:] == 'AW-R'):\n",
    "                            links.append(l)\n",
    "                except:\n",
    "                    continue\n",
    "            self.browser.find_element_by_partial_link_text('Show more').click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "        #eliminate link duplicates        \n",
    "        links = set(links)\n",
    "        \n",
    "        print('Number of posts being scraped: ', len(links))\n",
    "        \n",
    "        for link in links:\n",
    "            time.sleep(1)\n",
    "            self.browser.get(link)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            #*****change class name to match that of the page source code\n",
    "            post = self.browser.find_element_by_class_name('msg') #get post content\n",
    "            time.sleep(1)\n",
    "            content.append(post.text)\n",
    "\n",
    "            for i in range(20):\n",
    "                try:\n",
    "                    x = self.browser.find_element_by_partial_link_text('View more').get_attribute('href') #gets link to more comments\n",
    "                except:\n",
    "                    break\n",
    "                else:\n",
    "                    #*****change class name to match that of the page source code\n",
    "                    comm = self.browser.find_elements_by_class_name('_55wr') #get first level comments\n",
    "\n",
    "                    for t in comm:\n",
    "                        content.append(t.text)\n",
    "\n",
    "                    r = self.browser.find_elements_by_partial_link_text('replies') #only collects if more than one reply, gathers likes, date posted\n",
    "\n",
    "                    reply_links = []\n",
    "\n",
    "                    for m in r:\n",
    "                        time.sleep(1)\n",
    "                        reply_links.append(m.get_attribute('href')) #get all reply conversations\n",
    "\n",
    "                    for lin in reply_links:\n",
    "                        time.sleep(1)\n",
    "                        self.browser.get(lin)\n",
    "                        #*****change class name to get replies\n",
    "                        rep = self.browser.find_elements_by_class_name('bd') #get the replies content\n",
    "\n",
    "                        for e in rep:\n",
    "                            content.append(e.text)\n",
    "                    \n",
    "                    time.sleep(1)\n",
    "                    self.browser.get(x) #get more comments \n",
    "                    time.sleep(1)\n",
    "\n",
    "        self.browser.close()\n",
    "        \n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================MAIN===================\n",
    "def main():\n",
    "#====================Login====================\n",
    "    username = input(\"Username: \") \n",
    "    password = getpass(\"password: \")\n",
    "    bot=FacebookBot()\n",
    "    bot.login(username, password)\n",
    "    \n",
    "#====================Get Content from page======================\n",
    "\n",
    "    #must pass an mbasic facebook url \n",
    "    comments = bot.getContent(\"https://mbasic.facebook.com/IRS/?refid=17&_ft_=mf_story_key.10157813581394735%3Atop_level_post_id.10157813581394735%3Atl_objid.10157813581394735%3Acontent_owner_id_new.247781869734%3Athrowback_story_fbid.10157813581394735%3Apage_id.247781869734%3Aphoto_id.10157813581394735%3Astory_location.4%3Astory_attachment_style.photo%3Atds_flgs.3%3Apage_insights.%7B%22247781869734%22%3A%7B%22page_id%22%3A247781869734%2C%22page_id_type%22%3A%22page%22%2C%22actor_id%22%3A247781869734%2C%22dm%22%3A%7B%22isShare%22%3A0%2C%22originalPostOwnerID%22%3A0%7D%2C%22psn%22%3A%22EntPhotoNodeBasedEdgeStory%22%2C%22post_context%22%3A%7B%22object_fbtype%22%3A22%2C%22publish_time%22%3A1590937243%2C%22story_name%22%3A%22EntPhotoNodeBasedEdgeStory%22%2C%22story_fbid%22%3A%5B10157813581394735%5D%7D%2C%22role%22%3A1%2C%22sl%22%3A4%2C%22targets%22%3A%5B%7B%22actor_id%22%3A247781869734%2C%22page_id%22%3A247781869734%2C%22post_id%22%3A10157813581394735%2C%22role%22%3A1%2C%22share_id%22%3A0%7D%5D%7D%7D%3Athid.247781869734%3A306061129499414%3A69%3A0%3A1593586799%3A6104849151741984672&__tn__=C-R\"\\\n",
    "                                       ,deep=30)\n",
    "    \n",
    "    #*****Change Storage path if preferred*****\n",
    "    # save the data\n",
    "    with open('Facebook_IRS_Content.json', 'w') as out_f:\n",
    "        json.dump(comments, out_f)\n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
